# This docker-compose file is provided as an example to create a Docker Swarm based MSActivator setup
version: "3.8"

services:
  msa-front:
    image: ubiqube/msa2-front:935d7a705eb5f533841cb1a11993068aa5c57748
    depends_on:
      - msa-api
      - msa-ui
      - camunda
      - msa-ai-ml
    healthcheck:
      test: ["CMD-SHELL", "curl -k --fail https://localhost"]
      timeout: 2s
      retries: 10
      interval: 10s
      start_period: 30s
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.manager==true"
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: ingress
      - target: 443
        published: 443
        protocol: tcp
        mode: ingress
      - target: 514
        published: 514
        protocol: udp
        mode: ingress
      - target: 162
        published: 162
        protocol: udp
        mode: ingress
      - target: 69
        published: 69
        protocol: udp
        mode: ingress
      - "5200-5200:5200-5200/udp"
    #
    # uncomment one of the 2 sections below when installing a custom certificate 
    # - Docker standard standalone installation
    #volumes:
    #    - "msa_front:/etc/nginx/ssl"
    # - Docker Swarm HA installation
    volumes:
        - "/mnt/NASVolume/msa_front/http_nginx.conf:/etc/nginx/conf.d/http_nginx.conf"
        - "/mnt/NASVolume/msa_front/privkey.pem:/etc/nginx/ssl/server.key"
        - "/mnt/NASVolume/msa_front/fullchain.pem:/etc/nginx/ssl/server.crt"
    
  msa-smtp:
    image: namshi/smtp
    environment:
      SMARTHOST_ADDRESS: smtp-relay.gmail.com
      SMARTHOST_PORT: 587
      MAILNAME: app.cloudclapp.com
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.manager==true"

  # PLAN B - NO BIG ANIMAL
  # db:
  #   image: ubiqube/msa2-db:9ab326b65e8d4080e62f209984dd282977180a5ea
  #   healthcheck:
  #     test: ["CMD-SHELL", "/usr/pgsql-12/bin/pg_isready -h localhost"]
  #     timeout: 20s
  #   environment:
  #     CAMUNDA_PASSWORD: camunda
  #     CAMUNDA_DB: process-engine
  #     CAMUNDA_USER: camunda
  #     KEY_VAULT_USER: key_vault
  #     KEY_VAULT_DB: key_vault
  #     PG_MODE: primary
  #     PG_PRIMARY_USER: postgres
  #     PG_PRIMARY_PASSWORD: my_db_password
  #     PG_USER: postgres
  #     PG_PASSWORD: my_db_password
  #     PG_DATABASE: POSTGRESQL
  #     PG_ROOT_PASSWORD: my_db_password
  #     PG_PRIMARY_PORT: 5432
  #     MAX_CONNECTIONS: 1600
  #   volumes:
  #     - "/mnt/NASVolume/msa_db:/pgsqldata/pgsql"
  #   deploy:
  #     replicas: 1
  #     placement:
  #       max_replicas_per_node: 1
  #       constraints:
  #         - "node.labels.manager==true"

  msa-api:
    image: ubiqube/msa2-api:28fb6b0897c21f5619423e713d6e65b5f5225571
    depends_on:
      - key-vault
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8480"]
    environment:
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
      - UBIQUBE_CAPTCHA_SECRET_KEY=6LcFwjQfAAAAAFQSLf1dKYPoGtmaLOwoCPSzGl5k
      - UBIQUBE_SUNCHRO=no
      - UBIQUBE_BYPASS_CAPTCHA=true
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_api_logs:/opt/wildfly/logs/processLog"
      - "/mnt/NASVolume/msa_api_keystore:/etc/pki/jentreprise"
    networks:
      default:
        aliases:
          - "msa_api"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
        constraints:
          - "node.labels.worker==apps"
    logging:
      driver: "json-file"
      options:
        mode: non-blocking
        max-buffer-size: "40m"
        max-size: "100m"
        max-file: "5"

#    extra_hosts:
#      db: 20.223.38.173
#      msa-es: ES_IP

  msa-ui:
    image: ubiqube/msa2-ui:f7bdb4cb469d31ff91e6e5fe0fd3d1cd1d55fbf0
    depends_on:
      - msa-api
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080"]
    environment:
    - FEATURE_ADMIN=true
    - FEATURE_REPOSITORY=true
    - FEATURE_CONNECTION_STATUS=true
    - FEATURE_ALARMS=true
    - FEATURE_LICENCE=true
    - FEATURE_TOPOLOGY=true
    - FEATURE_MONITORING_PROFILES=true
    - FEATURE_PROFILE_AUDIT_LOGS=true
    - FEATURE_SCHEDULE_WORKFLOWS=true
    - FEATURE_PERMISSION_PROFILES=true
    - FEATURE_BPM=true
    - FEATURE_AI_ML=true
    - FEATURE_MICROSERVICE_BULK_OPERATION=true
    - FEATURE_EDIT_VARIABLES_IN_MICROSERVICE_CONSOLE=true
    - FEATURE_WORKFLOW_OWNER=false
    - FEATURE_PERMISSION_PROFILE_LABELS=false
    networks:
      default:
        aliases:
          - "msa_ui"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.worker==apps"

  cloudclapp:
    depends_on:
      - msa-api
    image: ubiqube/cloudclapp:f7bdb4cb469d31ff91e6e5fe0fd3d1cd1d55fbf0
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080"]
    environment:
      - UBIQUBE_CAPTCHA_SITE_KEY=6LcFwjQfAAAAAICJb7KjGjfKay4O3sOPaOb0glie
      - UBIQUBE_LICENSE_AGREEMENT_LINK=https:\/\/cloudclapp.com\/EndUserLicenceAgreement.html
      - UBIQUBE_HUBSPOT_TRACKING_ID=6638470
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.worker==apps"

  ccla-scan-app:
    image: ubiqube/cloudclapp-scan:eab7f66cf538fd6ae00775d397d13d58041b3760
    environment:
      - UBIQUBE_ZAP_TOKEN=7da091fe-63a4-48c0-9bfa-7614c49feb7c
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"

  ccla-scan-env:
    image: owasp/zap2docker-stable
    entrypoint:
      - zap.sh
      - -daemon
      - -host
      - 0.0.0.0
      - -config
      - api.addrs.addr.name=.*
      - -config
      - api.addrs.addr.regex=true
      - -config
      - api.key=7da091fe-63a4-48c0-9bfa-7614c49feb7c
      - -config
      - network.localServers.mainProxy.alpn.enabled=false
      - -config
      - network.localServers.mainProxy.address=0.0.0.0
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"

  msa-sms:
    image: ubiqube/msa2-sms:2ccddee00915beced393a989d7c21b6569af07cb
    # depends_on:
    #   - db
    healthcheck:
      test: ["CMD-SHELL", "/etc/init.d/ubi-sms status | grep -q 'service seems UP' || exit 1"]
    environment:
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
      - CONTAINER_DOCKNAME={{.Task.Name}}.{{.Node.Hostname}}
      - UBIQUBE_INSTALL_AWS_CLI=true
    hostname: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
      - "/mnt/NASVolume/msa_bulkfiles:/opt/sms/spool/parser"
      - "/mnt/NASVolume/msa_sms_logs:/opt/sms/logs"
    networks:
      default:
        aliases:
          - "msa_sms"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.worker==apps"
#    extra_hosts:
#      db: 20.223.38.173

  msa-bud:
    image: ubiqube/msa2-bud:9b7613ce285b4ec364aee6b384802d2bb96e806f
    # depends_on:
    #   - db
    healthcheck:
      test: ["CMD-SHELL", "/etc/init.d/ubi-bud status | grep -q 'service seems UP' || exit 1"]
    networks:
      default:
        aliases:
          - "msa_bud"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      db: 20.223.38.173

  msa-alarm:
    depends_on:
    #  - db
      - msa-api
    image: ubiqube/msa2-alarm:98f8f47175525f36a696a8d8382d1f3289839106
    healthcheck:
      test: ["CMD-SHELL", "/etc/init.d/ubi-alarm status | grep -q 'service seems UP' || exit 1"]
    environment:
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
      - CONTAINER_DOCKNAME={{.Task.Name}}.{{.Node.Hostname}}
    hostname: "{{.Task.Name}}.{{.Node.Hostname}}"
    volumes:
      - "/mnt/NASVolume/msa_sms_logs:/opt/sms/logs"
    networks:
      default:
        aliases:
          - "msa_alarm"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      db: 20.223.38.173

  msa-monitoring:
    image: ubiqube/msa2-monitoring:94e7b1355f3fa4ba2959b529fcff40e9876b78c4
    healthcheck:
      test: ["CMD-SHELL", "/etc/init.d/ubi-poll status | grep -q 'service seems UP' || exit 1"]
    depends_on:
    #  - db
      - msa-es
      - msa-dev
      - msa-api
    environment:
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
    volumes:
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/rrd_repository:/opt/rrd"
      - "/mnt/NASVolume/msa_bulkfiles:/opt/sms/spool/parser"
      - "/mnt/NASVolume/msa_sms_logs:/opt/sms/logs"
    networks:
      default:
        aliases:
          - "msa_monitoring"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      db: 20.223.38.173
#      msa-es: ES_IP

  camunda:
    # depends_on:
    #   - db
    image: ubiqube/msa2-camunda:23335a0df397d4bc3531366223cd5a239688ee0d
    environment:
      DB_DRIVER: org.postgresql.Driver
      DB_URL: 'jdbc:postgresql://db:5432/process-engine'
      DB_USERNAME: camunda
      DB_PASSWORD: camunda
      DB_VALIDATE_ON_BORROW: 'true'
      WAIT_FOR: 'db:5432'
      WAIT_FOR_TIMEOUT: 60
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      db: 20.223.38.173

  msa-es:
    image: ubiqube/msa2-es:7b0c2d96ae426da44b255efad0b1a60332f9c385
    #healthcheck:
    #  test: ["CMD-SHELL", "test -f /home/install/init-done && curl -s -XGET -H 'Authorization: Basic c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc='  'http://localhost:9200/_cluster/health?pretty' | grep -q 'status.*green' || exit 1"]
    #  timeout: 2s
    #  retries: 10
    #  interval: 10s
    #  start_period: 30s 
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
    environment:
      - discovery.type=single-node
      - script.painless.regex.enabled=true
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - "/mnt/NASVolume/msa_es:/usr/share/elasticsearch/data"
    networks:
      default:
        aliases:
          - "msa_es"

  msa-kibana:
    image: ubiqube/msa2-kibana:b5a0a64970f780ffddaf884f6bbff149431f0026
    depends_on:
      - msa-es
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_URL=http://msa_es:9200
      - ELASTICSEARCH_HOSTS=http://msa_es:9200
      - ES_CREDENTIALS=c3VwZXJ1c2VyOnheWnl1R002fnU9K2ZZMkc=
    networks:
      default:
        aliases:
          - "msa_kibana"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      msa-es: ES_IP

  msa-ai-ml:
    image: ubiqube/msa2-ai-ml:63c7fab8c111b6cc85da049f45ebc6175a9b269a
    healthcheck:
      test: ["CMD-SHELL", "python /msa_proj/health_check.py"]
    ports:
      - "8000:8000"
    volumes:
      - "/mnt/NASVolume/msa_ai_ml_db:/msa_proj/database"
    networks:
      default:
        aliases:
          - "msa_ai_ml"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"

  msa-cerebro:
    image: ubiqube/msa2-cerebro:914750e000db1343d9972bfa6652da1efe4aa32f
    environment:
      AUTH_TYPE: basic
      BASIC_AUTH_USER: cerebro
      BASIC_AUTH_PWD: "N@X{M4tfw'5%)+35"
    entrypoint:
      - /opt/cerebro/bin/cerebro
      - -Dhosts.0.host=http://msa_es:9200
    depends_on:
      - msa_es
    ports:
      - "9000:9000"
    networks:
      default:
        aliases:
          - "msa_cerebro"
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      msa-es: ES_IP

  msa-dev:
    image: ubiqube/msa2-linuxdev:d0055a9eb5f35669f88ed06b92005efe3b1c9989
    depends_on:
      - msa-es
    volumes:
      - "/mnt/NASVolume/msa_entities:/opt/fmc_entities"
      - "/mnt/NASVolume/msa_repository:/opt/fmc_repository"
      - "/mnt/NASVolume/msa_dev:/opt/devops/"
      - "/mnt/NASVolume/msa_svn:/opt/svnroot"
    networks:
      default:
        aliases:
          - "msa_dev"
    command: [ "/docker-entrypoint.sh", "--cloudclapp" ]
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.apps==true"
#    extra_hosts:
#      msa-es: ES_IP

  key-vault:
#    depends_on:
#      - db
    image: vault:latest
    ports:
      - "8200:8200"
    volumes:
      - /mnt/NASVolume/msa_key_vault/config.hcl:/vault/config/config.hcl
    environment:
      VAULT_ADDR: "http://0.0.0.0:8200"
    cap_add:
      - IPC_LOCK
    command: server
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 1
#        constraints:
#          - "node.labels.manager==true"
#    extra_hosts:
#      db: 20.223.38.173


networks:
  default:
    # driver_opts:
    #   encrypted: "true"
